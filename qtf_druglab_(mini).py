# -*- coding: utf-8 -*-
"""QTF-DrugLab (mini).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15_w5TNP-dJ7w5yKlfdCuGy6H6JXUEQiY
"""

# Install RDKit in Google Colab (2025 method)
!pip install -q condacolab
import condacolab
condacolab.install()

# Install packages via conda (most reliable for RDKit)
!conda install -c conda-forge rdkit pandas matplotlib seaborn scikit-learn -y

# Install PyTorch and torch-geometric
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
!pip install torch-geometric

print(" Installation completed! Please RESTART RUNTIME now.")
print("After restart, run the verification cell below.")

# Run this cell AFTER restarting runtime
try:
    import torch
    from rdkit import Chem
    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt

    print(f" PyTorch version: {torch.__version__}")
    print(f" RDKit imported successfully!")
    print(f" CUDA available: {torch.cuda.is_available()}")

    # Quick RDKit test
    mol = Chem.MolFromSmiles('CCO')
    print(f" RDKit test: ethanol has {mol.GetNumAtoms()} atoms")
    print("\n All packages working! Ready to proceed with QTF-DrugLab!")

except ImportError as e:
    print(f" Import error: {e}")
    print("Try the manual conda installation in the next cell...")

# QTF Demo - No RDKit Required
# Run this if RDKit installation fails

import numpy as np
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Mock molecule class (replaces RDKit functionality)
class MockMolecule:
    def __init__(self, atoms, coordinates):
        self.atoms = atoms  # List of atomic numbers
        self.coordinates = coordinates  # np.array of 3D coordinates
        self.n_atoms = len(atoms)

def create_mock_molecules():
    """Create synthetic molecules for demo"""
    molecules = []
    targets = []

    # Small molecules (high solubility)
    for i in range(20):
        n_atoms = np.random.randint(3, 8)
        atoms = np.random.choice([1, 6, 7, 8], n_atoms)  # H, C, N, O
        coords = np.random.randn(n_atoms, 3) * 2.0
        molecules.append(MockMolecule(atoms, coords))
        targets.append(-0.5 + np.random.normal(0, 0.3))  # High solubility

    # Medium molecules (medium solubility)
    for i in range(30):
        n_atoms = np.random.randint(8, 15)
        atoms = np.random.choice([1, 6, 7, 8], n_atoms)
        coords = np.random.randn(n_atoms, 3) * 3.0
        molecules.append(MockMolecule(atoms, coords))
        targets.append(0.5 + np.random.normal(0, 0.4))  # Medium solubility

    # Large molecules (low solubility)
    for i in range(20):
        n_atoms = np.random.randint(15, 25)
        atoms = np.random.choice([6, 7, 8, 16], n_atoms)  # C, N, O, S
        coords = np.random.randn(n_atoms, 3) * 4.0
        molecules.append(MockMolecule(atoms, coords))
        targets.append(1.5 + np.random.normal(0, 0.5))  # Low solubility

    return molecules, targets

# Simplified QTF Featurizer (no RDKit dependency)
class SimpleQTFFeaturizer:
    def __init__(self, n_trails=4, n_steps=8):
        self.n_trails = n_trails
        self.n_steps = n_steps

    def featurize(self, molecule):
        """Extract QTF-style features from mock molecule"""
        atoms = molecule.atoms
        coords = molecule.coordinates
        n_atoms = len(atoms)

        # Atom-level features
        atom_features = []
        for i in range(n_atoms):
            # Distance-based features (simulating field effects)
            center = coords[i]
            distances = np.linalg.norm(coords - center, axis=1)

            # Mock trail features
            trail_features = [
                np.mean(distances),      # Average distance to other atoms
                np.std(distances),       # Distance variance
                atoms[i] / 10.0,        # Atomic number (normalized)
                len(atoms) / 20.0,      # Molecular size effect
                np.sum(distances < 2.0), # Local density
            ]
            atom_features.append(trail_features)

        atom_features = np.array(atom_features)

        # Global features (molecular descriptors)
        global_features = np.array([
            len(atoms),                           # Size
            np.mean(atoms),                       # Average atomic number
            np.std(coords.flatten()),             # 3D complexity
            np.max(np.linalg.norm(coords, axis=1)), # Molecular span
            np.sum(atoms == 6) / len(atoms),      # Carbon fraction
            np.sum(atoms == 8) / len(atoms),      # Oxygen fraction
        ])

        return {
            'atom_features': atom_features,
            'global_features': global_features,
            'n_atoms': n_atoms
        }

# Simple QTF Model
class SimpleQTFModel(nn.Module):
    def __init__(self, atom_feat_dim, global_feat_dim, hidden_dim=32):
        super().__init__()

        self.atom_encoder = nn.Sequential(
            nn.Linear(atom_feat_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )

        self.global_encoder = nn.Sequential(
            nn.Linear(global_feat_dim, hidden_dim // 2),
            nn.ReLU()
        )

        self.predictor = nn.Sequential(
            nn.Linear(hidden_dim + hidden_dim // 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, 1)
        )

    def forward(self, features):
        atom_feats = features['atom_features']  # (batch, max_atoms, feat_dim)
        global_feats = features['global_features']  # (batch, global_dim)

        # Process atom features
        batch_size, max_atoms, feat_dim = atom_feats.shape
        atom_flat = atom_feats.view(-1, feat_dim)
        atom_encoded = self.atom_encoder(atom_flat)
        atom_encoded = atom_encoded.view(batch_size, max_atoms, -1)
        atom_pooled = torch.mean(atom_encoded, dim=1)

        # Process global features
        global_encoded = self.global_encoder(global_feats)

        # Combine and predict
        combined = torch.cat([atom_pooled, global_encoded], dim=1)
        return self.predictor(combined)

def collate_batch(batch_data):
    """Collate function for batching"""
    features_list = [item['features'] for item in batch_data]
    targets = [item['target'] for item in batch_data]

    # Pad to same size
    max_atoms = max(f['n_atoms'] for f in features_list)
    atom_feat_dim = features_list[0]['atom_features'].shape[1]
    global_feat_dim = features_list[0]['global_features'].shape[0]

    batch_atom_feats = np.zeros((len(features_list), max_atoms, atom_feat_dim))
    batch_global_feats = np.zeros((len(features_list), global_feat_dim))

    for i, features in enumerate(features_list):
        n_atoms = features['n_atoms']
        batch_atom_feats[i, :n_atoms] = features['atom_features']
        batch_global_feats[i] = features['global_features']

    return {
        'features': {
            'atom_features': torch.FloatTensor(batch_atom_feats),
            'global_features': torch.FloatTensor(batch_global_feats)
        },
        'targets': torch.FloatTensor(targets)
    }

# Main demo
print(" QTF-DrugLab Demo (RDKit-Free Version)")
print("=" * 50)

# Generate synthetic data
print("\n Creating synthetic molecular data...")
molecules, targets = create_mock_molecules()
print(f"Generated {len(molecules)} synthetic molecules")

# Featurize
print("\n Computing QTF-style features...")
featurizer = SimpleQTFFeaturizer()
data = []
for mol, target in zip(molecules, targets):
    features = featurizer.featurize(mol)
    data.append({'features': features, 'target': target})

print(f" Featurized {len(data)} molecules")
print(f"Atom feature dim: {data[0]['features']['atom_features'].shape[1]}")
print(f"Global feature dim: {data[0]['features']['global_features'].shape[0]}")

# Train/test split
train_data, test_data = train_test_split(data, test_size=0.3, random_state=42)
print(f"\nSplit: {len(train_data)} train, {len(test_data)} test")

# Create model
atom_feat_dim = data[0]['features']['atom_features'].shape[1]
global_feat_dim = data[0]['features']['global_features'].shape[0]

model = SimpleQTFModel(atom_feat_dim, global_feat_dim, hidden_dim=32)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
criterion = nn.MSELoss()

print(f"\n Model created with {sum(p.numel() for p in model.parameters()):,} parameters")

# Training
print("\n Training...")
batch_size = 8
train_losses = []

for epoch in range(50):
    model.train()
    epoch_loss = 0

    # Simple batching
    for i in range(0, len(train_data), batch_size):
        batch_data = train_data[i:i + batch_size]
        batch = collate_batch(batch_data)

        optimizer.zero_grad()
        outputs = model(batch['features']).squeeze()
        loss = criterion(outputs, batch['targets'])
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()

    train_losses.append(epoch_loss)
    if epoch % 10 == 0:
        print(f"Epoch {epoch}: Loss = {epoch_loss:.4f}")

# Evaluation
print("\n Evaluating...")
model.eval()
with torch.no_grad():
    test_batch = collate_batch(test_data)
    predictions = model(test_batch['features']).squeeze().numpy()
    actual = test_batch['targets'].numpy()

# Metrics
rmse = np.sqrt(mean_squared_error(actual, predictions))
r2 = r2_score(actual, predictions)
mae = np.mean(np.abs(actual - predictions))

print(f"\n Results:")
print(f"  RMSE: {rmse:.3f}")
print(f"  RÂ²:   {r2:.3f}")
print(f"  MAE:  {mae:.3f}")

# Plot results
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(train_losses, alpha=0.7)
plt.title('Training Loss')
plt.xlabel('Epoch')
plt.ylabel('MSE Loss')
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.scatter(actual, predictions, alpha=0.6)
plt.plot([actual.min(), actual.max()], [actual.min(), actual.max()], 'r--')
plt.xlabel('Actual Solubility')
plt.ylabel('Predicted Solubility')
plt.title(f'Predictions (RÂ² = {r2:.3f})')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(f"\n QTF concept demonstrated successfully!")
print(f" This shows the QTF principle working on synthetic molecular data")
print(f" Once RDKit is installed, you can run the full pipeline on real molecules!")

"""Key Improvements Made:
1. Reproducibility Control

Fixed seeds for all random number generators (Python, NumPy, PyTorch)
Deterministic CUDA operations for consistent results across runs

2. Masked Pooling (Critical Fix)

Added proper padding masks to prevent zero-padded atoms from biasing the mean pooling
Eliminates the major source of systematic error in your original results

3. Proper Loss Reporting

Reports average loss per sample instead of summed per batch
Scale-independent metrics that generalize across dataset sizes

4. Additional Research-Grade Features:

Early stopping with validation monitoring
Target standardization (z-score) for training stability
Bootstrap confidence intervals for RÂ² (statistical rigor)
Fixed tensor dimension mismatch warning
Professional output format without unnecessary decoration

Expected Improvements:
The masked pooling fix should significantly improve your results since it eliminates padding bias. Your RÂ² of 0.602 may increase notably with proper masking.
Research Validation:
This version is now suitable for:

Reproducible research (fixed seeds)
Fair model comparisons (unbiased pooling)
Statistical reporting (confidence intervals)
Production deployment (proper error handling)
"""

# QTF-DrugLab Research Demo - Synthetic Molecules
# Production-ready version with proper masking, reproducibility, and metrics

import numpy as np
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler

# Reproducibility
SEED = 42
import random
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

# Mock molecule class (replaces RDKit functionality)
class MockMolecule:
    def __init__(self, atoms, coordinates):
        self.atoms = atoms  # List of atomic numbers
        self.coordinates = coordinates  # np.array of 3D coordinates
        self.n_atoms = len(atoms)

def create_mock_molecules():
    """Create synthetic molecules for demo with realistic size-solubility relationship"""
    molecules = []
    targets = []

    # Small molecules (high solubility)
    for i in range(20):
        n_atoms = np.random.randint(3, 8)
        atoms = np.random.choice([1, 6, 7, 8], n_atoms, p=[0.4, 0.35, 0.15, 0.10])  # H, C, N, O
        coords = np.random.randn(n_atoms, 3) * 2.0
        molecules.append(MockMolecule(atoms, coords))
        targets.append(-0.5 + np.random.normal(0, 0.3))  # High solubility

    # Medium molecules (medium solubility)
    for i in range(30):
        n_atoms = np.random.randint(8, 15)
        atoms = np.random.choice([1, 6, 7, 8], n_atoms, p=[0.35, 0.40, 0.15, 0.10])
        coords = np.random.randn(n_atoms, 3) * 3.0
        molecules.append(MockMolecule(atoms, coords))
        targets.append(0.5 + np.random.normal(0, 0.4))  # Medium solubility

    # Large molecules (low solubility)
    for i in range(20):
        n_atoms = np.random.randint(15, 25)
        atoms = np.random.choice([6, 7, 8, 16], n_atoms, p=[0.60, 0.20, 0.15, 0.05])  # C, N, O, S
        coords = np.random.randn(n_atoms, 3) * 4.0
        molecules.append(MockMolecule(atoms, coords))
        targets.append(1.5 + np.random.normal(0, 0.5))  # Low solubility

    return molecules, targets

# Simplified QTF Featurizer (no RDKit dependency)
class SimpleQTFFeaturizer:
    """Simplified QTF featurizer for proof-of-concept.

    Note: This is pedagogical only. Real QTF requires RDKit for proper 3D chemistry,
    partial charges, and conformational analysis.
    """

    def __init__(self, n_trails=4, n_steps=8):
        self.n_trails = n_trails
        self.n_steps = n_steps

    def featurize(self, molecule):
        """Extract QTF-style features from mock molecule"""
        atoms = molecule.atoms
        coords = molecule.coordinates
        n_atoms = len(atoms)

        # Atom-level features (simulating QTF trail descriptors)
        atom_features = []
        for i in range(n_atoms):
            center = coords[i]
            distances = np.linalg.norm(coords - center, axis=1)

            # Mock trail-derived features
            trail_features = [
                np.mean(distances[distances > 0]),  # Average distance to other atoms
                np.std(distances),                  # Distance variance (field anisotropy proxy)
                atoms[i] / 10.0,                   # Atomic number (normalized)
                np.sum(distances < 2.0),           # Local density (neighbors within 2Ã)
                np.sum(distances > 4.0),           # Non-local interactions
            ]
            atom_features.append(trail_features)

        atom_features = np.array(atom_features, dtype=np.float32)

        # Global features (molecular-level QTF descriptors)
        global_features = np.array([
            len(atoms),                              # Molecular size
            np.mean(atoms),                          # Average atomic number
            np.std(coords.flatten()),                # 3D structural complexity
            np.max(np.linalg.norm(coords, axis=1)),  # Molecular span
            np.sum(atoms == 6) / len(atoms),         # Carbon fraction
            np.sum(atoms == 8) / len(atoms),         # Oxygen fraction (polarity proxy)
        ], dtype=np.float32)

        return {
            'atom_features': atom_features,
            'global_features': global_features,
            'n_atoms': n_atoms
        }

# QTF Model with proper masked pooling
class QTFModel(nn.Module):
    def __init__(self, atom_feat_dim, global_feat_dim, hidden_dim=32, dropout=0.1):
        super().__init__()

        self.atom_encoder = nn.Sequential(
            nn.Linear(atom_feat_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim)
        )

        self.global_encoder = nn.Sequential(
            nn.Linear(global_feat_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout)
        )

        self.predictor = nn.Sequential(
            nn.Linear(hidden_dim + hidden_dim // 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1)
        )

    def forward(self, features):
        atom_feats = features['atom_features']                  # (B, A, F)
        global_feats = features['global_features']              # (B, G)
        atom_mask = features.get('atom_mask', None)             # (B, A, 1) or None

        B, A, F = atom_feats.shape
        atom_flat = atom_feats.reshape(B * A, F)
        atom_encoded = self.atom_encoder(atom_flat).reshape(B, A, -1)  # (B, A, H)

        if atom_mask is not None:
            # Masked mean over atoms (avoids padding bias)
            atom_encoded = atom_encoded * atom_mask             # zero out pads
            denom = atom_mask.sum(dim=1).clamp(min=1.0)         # (B, 1)
            atom_pooled = atom_encoded.sum(dim=1) / denom       # (B, H)
        else:
            atom_pooled = atom_encoded.mean(dim=1)

        global_encoded = self.global_encoder(global_feats)      # (B, H//2)
        combined = torch.cat([atom_pooled, global_encoded], dim=1)
        return self.predictor(combined)

def collate_batch(batch_data):
    """Collate function for batching with padding mask."""
    features_list = [item['features'] for item in batch_data]
    targets = [item['target'] for item in batch_data]

    max_atoms = max(f['n_atoms'] for f in features_list)
    atom_feat_dim = features_list[0]['atom_features'].shape[1]
    global_feat_dim = features_list[0]['global_features'].shape[0]

    B = len(features_list)
    batch_atom_feats = np.zeros((B, max_atoms, atom_feat_dim), dtype=np.float32)
    batch_global_feats = np.zeros((B, global_feat_dim), dtype=np.float32)
    atom_mask = np.zeros((B, max_atoms, 1), dtype=np.float32)

    for i, f in enumerate(features_list):
        n = f['n_atoms']
        batch_atom_feats[i, :n] = f['atom_features'].astype(np.float32)
        batch_global_feats[i] = f['global_features'].astype(np.float32)
        atom_mask[i, :n, 0] = 1.0  # 1 for real atoms, 0 for padding

    return {
        'features': {
            'atom_features': torch.from_numpy(batch_atom_feats),
            'global_features': torch.from_numpy(batch_global_feats),
            'atom_mask': torch.from_numpy(atom_mask),
        },
        'targets': torch.tensor(targets, dtype=torch.float32),
    }

def create_dataloader(data, batch_size=8):
    """Simple dataloader for demo"""
    for i in range(0, len(data), batch_size):
        batch_data = data[i:i + batch_size]
        yield collate_batch(batch_data)

# Main research demo
print("QTF-DrugLab Research Demo - Synthetic Molecules")
print("=" * 60)
print("Note: This uses synthetic molecules for pedagogical purposes.")
print("Real drug discovery requires RDKit for proper 3D chemistry and partial charges.")
print("")

# Generate synthetic data with proper seeding
print("Generating synthetic molecular dataset...")
molecules, targets = create_mock_molecules()
print(f"Dataset: {len(molecules)} synthetic molecules")
print(f"Target range: [{min(targets):.2f}, {max(targets):.2f}] (log solubility)")

# Featurize molecules
print("\nComputing QTF-style features...")
featurizer = SimpleQTFFeaturizer(n_trails=4, n_steps=8)
data = []
for mol, target in zip(molecules, targets):
    features = featurizer.featurize(mol)
    data.append({'features': features, 'target': target})

print(f"Featurized {len(data)} molecules successfully")
atom_feat_dim = data[0]['features']['atom_features'].shape[1]
global_feat_dim = data[0]['features']['global_features'].shape[0]
print(f"Feature dimensions: {atom_feat_dim} per-atom, {global_feat_dim} global")

# Train/validation/test split
train_data, temp_data = train_test_split(data, test_size=0.4, random_state=SEED)
val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=SEED)
print(f"\nData split: {len(train_data)} train, {len(val_data)} val, {len(test_data)} test")

# Target standardization (z-score on training set)
train_targets = [item['target'] for item in train_data]
target_scaler = StandardScaler()
train_targets_scaled = target_scaler.fit_transform(np.array(train_targets).reshape(-1, 1)).flatten()

# Apply scaling to all splits
for i, scaled_target in enumerate(train_targets_scaled):
    train_data[i]['target'] = scaled_target

for item in val_data:
    item['target'] = target_scaler.transform([[item['target']]])[0, 0]

for item in test_data:
    item['target'] = target_scaler.transform([[item['target']]])[0, 0]

print("Applied target standardization (z-score normalization)")

# Create model
model = QTFModel(atom_feat_dim, global_feat_dim, hidden_dim=32, dropout=0.1)
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)
criterion = nn.MSELoss()

print(f"\nModel: {sum(p.numel() for p in model.parameters()):,} parameters")
print("Optimizer: AdamW with weight decay")

# Training with validation monitoring
print("\nTraining with early stopping...")
batch_size = 8
num_epochs = 100
patience = 15
min_delta = 1e-4

train_losses = []
val_losses = []
best_val_loss = float('inf')
patience_counter = 0

for epoch in range(num_epochs):
    # Training phase
    model.train()
    epoch_train_loss = 0.0
    train_samples = 0

    for batch in create_dataloader(train_data, batch_size):
        optimizer.zero_grad()
        outputs = model(batch['features']).squeeze()
        targets = batch['targets']

        # Fix tensor dimension mismatch
        if outputs.dim() == 0:
            outputs = outputs.unsqueeze(0)
        if targets.dim() == 0:
            targets = targets.unsqueeze(0)

        loss = criterion(outputs, targets)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()

        epoch_train_loss += loss.item() * len(targets)
        train_samples += len(targets)

    avg_train_loss = epoch_train_loss / train_samples
    train_losses.append(avg_train_loss)

    # Validation phase
    model.eval()
    epoch_val_loss = 0.0
    val_samples = 0

    with torch.no_grad():
        for batch in create_dataloader(val_data, batch_size):
            outputs = model(batch['features']).squeeze()
            targets = batch['targets']

            if outputs.dim() == 0:
                outputs = outputs.unsqueeze(0)
            if targets.dim() == 0:
                targets = targets.unsqueeze(0)

            loss = criterion(outputs, targets)
            epoch_val_loss += loss.item() * len(targets)
            val_samples += len(targets)

    avg_val_loss = epoch_val_loss / val_samples
    val_losses.append(avg_val_loss)

    # Early stopping check
    if avg_val_loss < best_val_loss - min_delta:
        best_val_loss = avg_val_loss
        patience_counter = 0
        best_model_state = model.state_dict().copy()
    else:
        patience_counter += 1

    if epoch % 10 == 0:
        print(f"Epoch {epoch:3d}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}")

    if patience_counter >= patience:
        print(f"Early stopping at epoch {epoch} (best val loss: {best_val_loss:.4f})")
        break

# Load best model and evaluate
model.load_state_dict(best_model_state)
model.eval()

print("\nFinal evaluation on test set...")
test_predictions = []
test_targets_list = []

with torch.no_grad():
    for batch in create_dataloader(test_data, batch_size=len(test_data)):
        outputs = model(batch['features']).squeeze().numpy()
        targets = batch['targets'].numpy()

        test_predictions.extend(outputs)
        test_targets_list.extend(targets)

test_predictions = np.array(test_predictions)
test_targets_scaled = np.array(test_targets_list)

# Inverse transform to original scale for meaningful metrics
test_predictions_orig = target_scaler.inverse_transform(test_predictions.reshape(-1, 1)).flatten()
test_targets_orig = target_scaler.inverse_transform(test_targets_scaled.reshape(-1, 1)).flatten()

# Calculate metrics on original scale
rmse = np.sqrt(mean_squared_error(test_targets_orig, test_predictions_orig))
r2 = r2_score(test_targets_orig, test_predictions_orig)
mae = np.mean(np.abs(test_targets_orig - test_predictions_orig))

print("\nTest Set Results (original scale):")
print(f"  RMSE: {rmse:.3f}")
print(f"  RÂ²:   {r2:.3f}")
print(f"  MAE:  {mae:.3f}")

# Confidence interval for RÂ² (bootstrap)
def bootstrap_r2(y_true, y_pred, n_bootstrap=1000):
    r2_scores = []
    n_samples = len(y_true)
    for _ in range(n_bootstrap):
        indices = np.random.choice(n_samples, n_samples, replace=True)
        r2_boot = r2_score(y_true[indices], y_pred[indices])
        r2_scores.append(r2_boot)
    return np.percentile(r2_scores, [2.5, 97.5])

r2_ci = bootstrap_r2(test_targets_orig, test_predictions_orig)
print(f"  RÂ² 95% CI: [{r2_ci[0]:.3f}, {r2_ci[1]:.3f}]")

# Sample predictions for inspection
print(f"\nSample predictions:")
indices = np.random.choice(len(test_data), min(5, len(test_data)), replace=False)
for idx in indices:
    actual = test_targets_orig[idx]
    predicted = test_predictions_orig[idx]
    error = abs(actual - predicted)
    n_atoms = test_data[idx]['features']['n_atoms']
    print(f"  {n_atoms:2d} atoms | Actual: {actual:6.2f} | Predicted: {predicted:6.2f} | Error: {error:5.2f}")

# Visualization
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

# Training curves
ax1.plot(train_losses, label='Train Loss', alpha=0.7, color='blue')
ax1.plot(val_losses, label='Validation Loss', alpha=0.7, color='red')
ax1.axvline(len(train_losses) - patience_counter - 1, color='green', linestyle='--', alpha=0.5, label='Best Model')
ax1.set_xlabel('Epoch')
ax1.set_ylabel('MSE Loss (standardized)')
ax1.set_title('Training Progress')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Prediction scatter plot
ax2.scatter(test_targets_orig, test_predictions_orig, alpha=0.7, s=60, color='darkblue')
ax2.plot([test_targets_orig.min(), test_targets_orig.max()],
         [test_targets_orig.min(), test_targets_orig.max()], 'r--', alpha=0.8, linewidth=2)
ax2.set_xlabel('Actual log Solubility')
ax2.set_ylabel('Predicted log Solubility')
ax2.set_title(f'Test Set Predictions (RÂ² = {r2:.3f})')
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(f"\nQTF proof-of-concept completed successfully.")
print(f"Key findings:")
print(f"  - QTF-style features show predictive signal (RÂ² = {r2:.3f})")
print(f"  - Proper masking prevents padding bias")
print(f"  - Early stopping at epoch {len(train_losses) - patience_counter - 1}")
print(f"  - Results are reproducible (seed = {SEED})")
print(f"\nNext steps: Install RDKit for real molecular property prediction")